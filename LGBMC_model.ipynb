{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LightGBM model training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv into a df\n",
    "csv_path = \"../data/processed/merged_data_base.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Drop id column\n",
    "df = df.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y df's\n",
    "X = df.drop(columns=['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56'])\n",
    "Y = df[['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = X.select_dtypes(include='object').columns.to_list()\n",
    "X = pd.get_dummies(X, columns=ordinal_cols, drop_first=False)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Relevance with LGBM\n",
    "LGBM_models = {}\n",
    "for i in Y_train.columns:\n",
    "    model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, Y_train[i])\n",
    "    LGBM_models[i] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "for col, model in LGBM_models.items():\n",
    "    predictions[col] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, predictions, target_names=Y.columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Resultant Model Eval.:  \n",
    "Average F1-Score: 0.99\n",
    "Lowest precision = 0.85 on B54B\n",
    "Lowest Recall = 0.73 on B54B\n",
    "Lowest F-1 = 0.85 on B354B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Model inference Eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "## Time in seconds for inferencing a batch of 10 instances\n",
    "start_time = time.time()\n",
    "model.predict(X_test[:10])\n",
    "inf_time = (time.time() - start_time) /10\n",
    "print(inf_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss # fraction of labesl incorrectly classified.\n",
    "print(hamming_loss(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # EXACT match ration == Accuracy %\n",
    "print(accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "conf = multilabel_confusion_matrix(Y_test, predictions)\n",
    "\n",
    "labels = ['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56']\n",
    "\n",
    "for i, mtx in enumerate(conf):\n",
    "    plt.figure()\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f\"confusion matrix for {labels[i]}\")\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
