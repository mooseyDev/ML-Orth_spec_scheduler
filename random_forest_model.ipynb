{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Random forest model training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Although the raw data has now been cleaned, each AI/ML Model we train needs to first process the data so it is formatted slightly differently. \n",
    "For Random Forest models, we need to impute missing numerical values with a neutral constant like -1 and for catagorical features we need to impute with a neutral catagory like \"missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Python 3.11, Conda environment.\n",
    "# Ensure using env.yml to create the environment (data_analysis_env). \n",
    "# Importing data analysis & visualisation librarys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv into a df\n",
    "csv_path = \"../data/processed/merged_data_base.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id column\n",
    "df = df.drop('id', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out columns as labels and columns that are features, create new X and Y for training\n",
    "label_cols = ['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56'] # create array with all labels\n",
    "features = [col for col in df.columns if col not in label_cols] # get all columns that arent in the label column array\n",
    "\n",
    "Y = df[label_cols].copy() # Y df becomes labels \n",
    "X = df[features].copy() # X df becomes the features\n",
    "\n",
    "# now identify which columns are catagorical and which are numerical\n",
    "ordinal_cols = X.select_dtypes(include='object').columns.to_list() # Get all columns that are object dtype\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.to_list() # Get all cols that are num dytpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputing\n",
    "X[num_cols] = X[num_cols].fillna(-1) # change num NaN's to -1\n",
    "X[ordinal_cols] = X[ordinal_cols].fillna('missing') # Change catagory NaN's to 'missing'\n",
    "\n",
    "# One-Hot encode catagorical columns using get_dummies()\n",
    "X = pd.get_dummies(X, columns=ordinal_cols, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up test and training splits\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100,\n",
    "                                                                   criterion= 'entropy', \n",
    "                                                                   random_state=42\n",
    "                                                                   ))\n",
    "random_forest_model.fit(X_train, Y_train)\n",
    "y_prediction = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_prediction, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature importance to ensure model is logically classifying the data based on domain knowledge\n",
    "feature_names = X_train.columns\n",
    "\n",
    "for i, n in enumerate(random_forest_model.estimators_): # Loop over each tree\n",
    "    importances = n.feature_importances_ # get the feature importances\n",
    "    label = Y_train.columns[i] # Get the target label name from corresponding index\n",
    "    sorted_id = np.argsort(importances)[::-1] # Sorts importance and store in variable\n",
    "    print(f\"\\ntop features for the label '{label}':\") #print header for each label\n",
    "    for idx in sorted_id[:10]: #iterate over top 10\n",
    "        print(f\"{feature_names[idx]} -> {importances[idx]}\") #print its importance score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Feature engineering of under-represented labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify under/over-representation of labels\n",
    "label_counts = df[label_cols].sum().sort_values()\n",
    "print(f\"Occurrence of each label (sorted):\\n{label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under 0.98 F1-Score labels were chosen\n",
    "B55B_pos = df[df['B55B'] == 1]\n",
    "B55B_oversample = pd.concat([B55B_pos] *3, ignore_index=True)\n",
    "\n",
    "B54B_pos = df[df['B54B'] == 1]\n",
    "B54B_oversample = pd.concat([B54B_pos] *3, ignore_index=True)\n",
    "\n",
    "B54A_pos = df[df['B54A'] == 1]\n",
    "B54A_oversample = pd.concat([B54A_pos] *3, ignore_index=True)\n",
    "\n",
    "B41_pos = df[df['B41'] == 1]\n",
    "B41_oversample = pd.concat([B41_pos] *3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an oversampled dataframe.\n",
    "df_oversample = pd.concat([df, B55B_oversample, B54B_oversample, B54A_oversample, B41_oversample], ignore_index=True)\n",
    "\n",
    "# Now retrain again using this new df\n",
    "label_cols = ['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56']\n",
    "features = [col for col in df_oversample.columns if col not in label_cols]\n",
    "\n",
    "Y = df_oversample[label_cols].copy()\n",
    "X = df_oversample[features].copy()\n",
    "\n",
    "# now identify which columns are catagorical and which are numerical\n",
    "ordinal_cols = X.select_dtypes(include='object').columns.to_list()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.to_list()\n",
    "\n",
    "# Missing value imputing\n",
    "X[num_cols] = X[num_cols].fillna(-1)\n",
    "X[ordinal_cols] = X[ordinal_cols].fillna('missing')\n",
    "\n",
    "# One-Hot encode catagorical columns using get_dummies()\n",
    "X = pd.get_dummies(X, columns=ordinal_cols, drop_first=False)\n",
    "\n",
    "# Set up test and training splits\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "random_forest_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100,\n",
    "                                                                   criterion='entropy', \n",
    "                                                                   random_state=42))\n",
    "random_forest_model.fit(X_train, Y_train)\n",
    "Y_prediction = random_forest_model.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, Y_prediction, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Resultant Model Eval.:  \n",
    "Average F1-Score: 1.00\n",
    "Lowest precision = 0.98 on B43\n",
    "Lowest Recall = 0.99 on B36\n",
    "Lowest F-1 = 0.99 on B36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Model inference Eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "## Time in seconds for inferencing a batch of 10 instances\n",
    "start_time = time.time()\n",
    "random_forest_model.predict(X_test[:10])\n",
    "inf_time = (time.time() - start_time) /10\n",
    "print(inf_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss # fraction of labesl incorrectly classified.\n",
    "print(hamming_loss(Y_test, Y_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # EXACT match ration == Accuracy %\n",
    "print(accuracy_score(Y_test, Y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "conf = multilabel_confusion_matrix(Y_test, Y_prediction)\n",
    "\n",
    "labels = ['B36', 'B41', 'B43', 'B54A', 'B54B', 'B54C', 'B55A', 'B55B', 'B56']\n",
    "\n",
    "for i, mtx in enumerate(conf):\n",
    "    plt.figure()\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f\"confusion matrix for {labels[i]}\")\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
